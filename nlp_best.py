# -*- coding: utf-8 -*-
"""nlp_best.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LBOkZqEYzVkNUB9AM9sxm653sqmG0Cgq
"""

# Importing Libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import nltk
import warnings
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(max_features=3000)

warnings.filterwarnings('ignore')

col_names = ['ID', 'Entity', 'Sentiment', 'Content']

train = pd.read_csv("/content/twitter_training.csv.zip",names=col_names)
train.sample(5)

test = pd.read_csv("/content/twitter_validation.csv",names=col_names)
test.sample(5)

train.shape

# Data Preprocessing

train.info()

# checking values
train.isnull().sum()

train.dropna(subset=['Content'], inplace=True)

# checking duplicate values
train.duplicated().sum()

# replacing irrelavant with neutral
train['Sentiment'] = train['Sentiment'].replace('Irrelevant', 'Neutral')
test['Sentiment'] = test['Sentiment'].replace('Irrelevant', 'Neutral')

train.head()

# dropping duplicates
train = train.drop_duplicates(keep='first')

train.duplicated().sum()

train.shape

train.head()

# Exploratory Data Analysis (EDA)

train['Sentiment'].value_counts()

plt.pie(train['Sentiment'].value_counts(), labels=['Neutral','Negative','Positive'],autopct='%0.2f',colors=['yellow','red','green'])
plt.show()

# Data is imbalanced
# hence we will give more importance to accuracy than precision

nltk.download('punkt')

train['num_char'] = train['Content'].apply(len) # no of characters of each text

train.head()

# number of words
train['num_words'] = train['Content'].apply(lambda x: len(nltk.word_tokenize(x)))

train.head()

# number of sentences
train['num_sentences'] = train['Content'].apply(lambda x: len(nltk.sent_tokenize(x)))

train.head()

# data description
train[['num_char','num_words','num_sentences']].describe()

plt.figure(figsize=(6,4))
sns.histplot(train[train['Sentiment'] == 'Neutral']['num_char'])
sns.histplot(train[train['Sentiment'] == 'Negative']['num_char'], color='red')
sns.histplot(train[train['Sentiment'] == 'Positive']['num_char'], color='green')

plt.figure(figsize=(6,4))
sns.histplot(train[train['Sentiment'] == 'Neutral']['num_words'])
sns.histplot(train[train['Sentiment'] == 'Negative']['num_words'], color='red')
sns.histplot(train[train['Sentiment'] == 'Positive']['num_words'], color='green')

df=train.drop(['ID','Entity'],axis=1)

df.head()

# Perform one-hot encoding
df_encoded = pd.get_dummies(df, columns=['Sentiment'],drop_first=True)
df_encoded['Sentiment_Neutral'] = df_encoded['Sentiment_Neutral'].astype(int)
df_encoded['Sentiment_Positive'] = df_encoded['Sentiment_Positive'].astype(int)

df_encoded=df_encoded.drop(['Content'],axis=1)

sns.heatmap(df_encoded.corr(),annot=True)

# Text Preprocessing
# Except for removing stopwords, we are doing the rest

import string # for punctuation
# string.punctuation
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

def transform_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)

    y = []
    for i in text:
        if i.isalnum():
            y.append(i)

    text = y[:] # list is mutable, so you have to do cloning, else if you clear y, text gets cleared too
    y.clear()

    for i in text:
        y.append(ps.stem(i))

    return " ".join(y)

# Perform Label encoding
label_encoder = LabelEncoder()
df['sentiment_encoded'] = label_encoder.fit_transform(df['Sentiment'])

df.head()

df['transformed_text'] = df['Content'].apply(transform_text)

df.head()

from wordcloud import WordCloud
wc = WordCloud(width=1000, height=1000, min_font_size=10, background_color='black')

condition = (df['sentiment_encoded']==0)

# Generate the word cloud for "transformed_text" when both conditions are met
wcNeutral = WordCloud().generate(df[condition]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(8,8))
plt.imshow(wcNeutral)

condition = (df['sentiment_encoded']==1)

# Generate the word cloud for "transformed_text" when both conditions are met
wcNegative = WordCloud().generate(df[condition]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(8,8))
plt.imshow(wcNegative)

condition = (df['sentiment_encoded']==2)

# Generate the word cloud for "transformed_text" when both conditions are met
wcPositive = WordCloud().generate(df[condition]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(8,8))
plt.imshow(wcPositive)

df=df.drop(['Sentiment'],axis=1)

X = tfidf.fit_transform(df['transformed_text']).toarray()

print(X.shape)

y = df['sentiment_encoded'].values

print(y.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

bnb = BernoulliNB()

# TFIDF Vectorizer - BernoulliNB
bnb.fit(X_train, y_train)
y_pred = bnb.predict(X_test)
print(precision_score(y_test,y_pred,average=None))

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Create a pipeline with just the logistic regression classifier
pipeline = Pipeline([
    ('lr_clf', LogisticRegression(solver='liblinear'))
])

# defining the hyperparameter grid for logistic regression (C parameter)
params = {'lr_clf__C': [1, 5, 10]}

grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='accuracy', verbose=1)
grid_cv_pipe.fit(X_train, y_train)
print('Optimized Hyperparameters:', grid_cv_pipe.best_params_)

pred = grid_cv_pipe.predict(X_test)
print('Optimized Accuracy Score: {0: .3f}'.format(accuracy_score(y_test, pred)))

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

# Now we repeat it while removing the stopwords

col_names = ['ID', 'Entity', 'Sentiment', 'Content']

train1 = pd.read_csv("/content/twitter_training.csv.zip",names=col_names)
train1.sample(5)

train = pd.read_csv("/content/twitter_training.csv.zip",names=col_names)
train.sample(5)

test = pd.read_csv("/content/twitter_training.csv.zip",names=col_names)
test.sample(5)

train.shape

# Data Preprocessing

train.info()

# checking values
train.isnull().sum()

train.dropna(subset=['Content'], inplace=True)

# checking duplicate values
train.duplicated().sum()

# replacing irrelavant with neutral
train['Sentiment'] = train['Sentiment'].replace('Irrelevant', 'Neutral')
test['Sentiment'] = test['Sentiment'].replace('Irrelevant', 'Neutral')

train.head()

# dropping duplicates
train = train.drop_duplicates(keep='first')

train.duplicated().sum()

train.shape

train.head()

# Exploratory Data Analysis (EDA)

train['Sentiment'].value_counts()

plt.pie(train['Sentiment'].value_counts(), labels=['Neutral','Negative','Positive'],autopct='%0.2f',colors=['yellow','red','green'])
plt.show()

# Data is imbalanced
# hence we will give more importance to accuracy than precision

nltk.download('punkt')

train['num_char'] = train['Content'].apply(len) # no of characters of each text

train.head()

# number of words
train['num_words'] = train['Content'].apply(lambda x: len(nltk.word_tokenize(x)))

train.head()

# number of sentences
train['num_sentences'] = train['Content'].apply(lambda x: len(nltk.sent_tokenize(x)))

train.head()

# data description
train[['num_char','num_words','num_sentences']].describe()

plt.figure(figsize=(6,4))
sns.histplot(train[train['Sentiment'] == 'Neutral']['num_char'])
sns.histplot(train[train['Sentiment'] == 'Negative']['num_char'], color='red')
sns.histplot(train[train['Sentiment'] == 'Positive']['num_char'], color='green')

plt.figure(figsize=(6,4))
sns.histplot(train[train['Sentiment'] == 'Neutral']['num_words'])
sns.histplot(train[train['Sentiment'] == 'Negative']['num_words'], color='red')
sns.histplot(train[train['Sentiment'] == 'Positive']['num_words'], color='green')

df=train.drop(['ID','Entity'],axis=1)

# Perform one-hot encoding
df_encoded = pd.get_dummies(df, columns=['Sentiment'],drop_first=True)
df_encoded['Sentiment_Neutral'] = df_encoded['Sentiment_Neutral'].astype(int)
df_encoded['Sentiment_Positive'] = df_encoded['Sentiment_Positive'].astype(int)

df_encoded=df_encoded.drop(['Content'],axis=1)

sns.heatmap(df_encoded.corr(),annot=True)

# Text Preprocessing

from nltk.corpus import stopwords
import string # for punctuation
# string.punctuation
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

def transform_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)

    y = []
    for i in text:
        if i.isalnum():
            y.append(i)

    text = y[:] # list is mutable, so you have to do cloning, else if you clear y, text gets cleared too
    y.clear()

    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        y.append(ps.stem(i))

    return " ".join(y)

# Perform Label encoding
label_encoder = LabelEncoder()
df['sentiment_encoded'] = label_encoder.fit_transform(df['Sentiment'])

df.head()

import nltk

nltk.download('stopwords')

nltk.download('stopwords', download_dir='/path/to/nltk_data')

df['transformed_text'] = df['Content'].apply(transform_text)

df.head()

from wordcloud import WordCloud
wc = WordCloud(width=1000, height=1000, min_font_size=10, background_color='black')

condition = (df['sentiment_encoded']==0)

# Generate the word cloud for "transformed_text" when both conditions are met
wcNeutral = WordCloud().generate(df[condition]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(8,8))
plt.imshow(wcNeutral)

condition = (df['sentiment_encoded']==1)

# Generate the word cloud for "transformed_text" when both conditions are met
wcNegative = WordCloud().generate(df[condition]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(8,8))
plt.imshow(wcNegative)

condition = (df['sentiment_encoded']==2)

# Generate the word cloud for "transformed_text" when both conditions are met
wcPositive = WordCloud().generate(df[condition]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(8,8))
plt.imshow(wcPositive)

df=df.drop(['Sentiment'],axis=1)

X = tfidf.fit_transform(df['transformed_text']).toarray()

print(X.shape)

y = df['sentiment_encoded'].values

print(y.shape)

from sklearn.model_selection import train_test_split

X_train1, X_test1, y_train1, y_test1 = train_test_split(X,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

bnb = BernoulliNB()

# TFIDF Vectorizer - BernoulliNB
bnb.fit(X_train1, y_train1)
y_pred1 = bnb.predict(X_test1)
print(precision_score(y_test1,y_pred1,average=None))

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Create a pipeline with just the logistic regression classifier
pipeline = Pipeline([
    ('lr_clf', LogisticRegression(solver='liblinear'))
])

# defining the hyperparameter grid for logistic regression (C parameter)
params = {'lr_clf__C': [1, 5, 10]}

grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='accuracy', verbose=1)
grid_cv_pipe.fit(X_train1, y_train1)
print('Optimized Hyperparameters:', grid_cv_pipe.best_params_)

pred1 = grid_cv_pipe.predict(X_test1)
print('Optimized Accuracy Score: {0: .3f}'.format(accuracy_score(y_test1, pred1)))

from sklearn.metrics import classification_report
print(classification_report(y_test1,y_pred1))

